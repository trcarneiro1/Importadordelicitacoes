# üöÄ Orquestrador de Scraping - Guia de Uso

## ‚úÖ O que foi criado

### 1. **Orquestrador** (`lib/scrapers/orchestrator.ts`)
Coordena o scraping de todas as 47 SREs com:
- ‚úÖ Retry logic (2 tentativas)
- ‚úÖ Rate limiting (2s entre requests)
- ‚úÖ Logs autom√°ticos no Supabase
- ‚úÖ Atualiza√ß√£o de status das SREs
- ‚úÖ Suporte a m√∫ltiplas URLs por SRE
- ‚úÖ Tratamento de casos especiais (Google Sites, etc)

### 2. **API de Teste** (`/api/test-scraper`)
Endpoint para testar scraping via HTTP

### 3. **Script CLI** (`scripts/test-scraper.js`)
Script para testes via terminal (requer compila√ß√£o)

---

## üß™ Como Testar

### M√©todo 1: Via API (Recomendado)

**Servidor j√° est√° rodando em:** `http://localhost:3001`

#### Testar uma SRE:
```bash
# Windows PowerShell
Invoke-WebRequest "http://localhost:3001/api/test-scraper?sre=6"

# Ou abra no navegador:
http://localhost:3001/api/test-scraper?sre=6
```

#### Testar m√∫ltiplas SREs:
```bash
# Testar Barbacena, Curvelo e Juiz de Fora
http://localhost:3001/api/test-scraper?sre=6,13,22
```

#### SREs Recomendadas para Teste:
- **6** - Barbacena (Joomla padr√£o)
- **13** - Curvelo (6 URLs diferentes)
- **14** - Diamantina (4 URLs)
- **22** - Juiz de Fora (Google Sites - caso especial)
- **25** - Montes Claros (Joomla padr√£o)
- **41** - Te√≥filo Otoni (Joomla + Google Sheets)

---

## üìä Resposta da API

### Sucesso (1 SRE):
```json
{
  "success": true,
  "result": {
    "sre_code": 6,
    "sre_name": "Barbacena",
    "success": true,
    "licitacoes_found": 15,
    "urls_scraped": 1,
    "duration_seconds": 8,
    "error": null
  }
}
```

### Sucesso (M√∫ltiplas SREs):
```json
{
  "success": true,
  "sres_tested": 3,
  "results": [
    {
      "sre_code": 6,
      "sre_name": "Barbacena",
      "success": true,
      "licitacoes_found": 15,
      "urls_scraped": 1,
      "duration_seconds": 8
    },
    {
      "sre_code": 13,
      "sre_name": "Curvelo",
      "success": true,
      "licitacoes_found": 23,
      "urls_scraped": 6,
      "duration_seconds": 42
    }
  ]
}
```

### Erro:
```json
{
  "error": "Invalid SRE code. Must be between 1 and 47."
}
```

---

## üîç Verificar Dados Coletados

### Via Supabase SQL Editor:
```sql
-- Ver licita√ß√µes coletadas recentemente
SELECT 
  sre_source,
  regional,
  COUNT(*) as total,
  MAX(created_at) as ultima_coleta
FROM licitacoes
GROUP BY sre_source, regional
ORDER BY ultima_coleta DESC;

-- Ver logs de scraping
SELECT 
  sre_source,
  status,
  licitacoes_coletadas,
  error_message,
  created_at
FROM scraping_logs
ORDER BY created_at DESC
LIMIT 20;

-- Ver status das SREs
SELECT 
  codigo,
  nome,
  ultima_coleta,
  taxa_sucesso,
  proxima_coleta
FROM sres
WHERE ultima_coleta IS NOT NULL
ORDER BY ultima_coleta DESC;
```

### Via API:
```bash
# Ver licita√ß√µes coletadas
http://localhost:3001/api/licitacoes

# Ver logs de scraping
http://localhost:3001/api/scraping-logs
```

---

## üöÄ Pr√≥ximos Passos

### 1. Testar 3 SREs Diversas
Execute agora:
```bash
# No navegador ou PowerShell:
http://localhost:3001/api/test-scraper?sre=6,13,22
```

### 2. Verificar Dados no Supabase
- Acesse Supabase ‚Üí Table Editor ‚Üí `licitacoes`
- Verifique se h√° novos registros com `sre_code = 6, 13, 22`

### 3. Criar GitHub Action
Ap√≥s validar que funciona:
```yaml
# .github/workflows/scrape-daily.yml
name: Daily Scraping
on:
  schedule:
    - cron: '0 6 * * *'  # 6 AM BRT todos os dias
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
      - run: npm install
      - run: npm run scrape:all
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_KEY }}
```

### 4. Agente de Enriquecimento IA
Criar `lib/agents/enrichment-agent.ts` para:
- Extrair escola, munic√≠pio, itens
- Gerar score de relev√¢ncia
- Criar resumo executivo
- Popular campos B2B

---

## üìù Notas T√©cnicas

### Rate Limiting
- 2 segundos entre cada URL
- Para SREs com m√∫ltiplas URLs (Curvelo = 6 URLs), leva ~12s

### Retry Logic
- 2 tentativas autom√°ticas
- Espera 5s antes de retry
- Salva erro no banco ap√≥s falhas

### Casos Especiais
- **Google Sites** (Juiz de Fora): Retorna erro "not yet supported"
- **Google Sheets** (Te√≥filo Otoni): Scraping parcial (apenas site Joomla)
- **M√∫ltiplas URLs** (Curvelo, Diamantina): Scraping sequencial de todas

### Performance
- **1 SRE**: ~8-10 segundos
- **10 SREs**: ~2-3 minutos
- **47 SREs**: ~10-15 minutos (com rate limiting)

---

## üêõ Troubleshooting

### Erro: "Missing Supabase credentials"
Verifique `.env.local`:
```env
NEXT_PUBLIC_SUPABASE_URL=https://xxx.supabase.co
SUPABASE_SERVICE_ROLE_KEY=eyJxxx...
```

### Erro: "Failed to fetch SREs"
Execute primeiro:
```bash
npm run import:sres
```

### Nenhuma licita√ß√£o encontrada
- Normal para sites vazios ou fora do ar
- Verifique logs: `SELECT * FROM scraping_logs ORDER BY created_at DESC;`

### Timeout
- Aumente timeout no `fetch()` (atualmente 30s padr√£o)
- Ou reduza n√∫mero de SREs testadas

---

## ‚úÖ Status Atual

- [x] Orquestrador criado
- [x] API de teste criada
- [x] Servidor dev rodando
- [ ] **AGORA:** Testar com 3 SREs (6, 13, 22)
- [ ] Validar dados no Supabase
- [ ] Criar GitHub Action
- [ ] Criar agente de enriquecimento IA

---

## üéØ Teste Agora!

Abra no navegador:
```
http://localhost:3001/api/test-scraper?sre=6
```

Ou teste m√∫ltiplas:
```
http://localhost:3001/api/test-scraper?sre=6,25,40
```

Aguarde 10-30 segundos e verifique a resposta JSON! üöÄ
