# ğŸ¯ PRÃ“XIMOS PASSOS - Importador de LicitaÃ§Ãµes

## ğŸ“Š SITUAÃ‡ÃƒO ATUAL

âœ… **ConcluÃ­do:**
- Prisma ORM instalado e configurado
- Schema com 5 tabelas (sres, licitacoes, scraping_logs, noticias, user_alerts)
- Database conectada ao Supabase PostgreSQL
- Setup executado: colunas adicionadas, Ã­ndices criados
- Orchestrator refatorado com Prisma (type-safe)
- Servidor rodando em http://localhost:3001

â³ **Pendente:**
- Validar scraping funcionando com Prisma
- Testar mÃºltiplas SREs
- Verificar dados salvos no banco

---

## ğŸš€ FASE 1: VALIDAÃ‡ÃƒO DO SCRAPING (HOJE)

### Passo 1.1: Testar Scraping Individual â±ï¸ 2 min
```powershell
# Teste simples - SRE 6 (Barbacena)
Start-Process "http://localhost:3001/api/test-scraper?sre=6"
```

**Resultado esperado:**
```json
{
  "success": true,
  "result": {
    "sre_code": 6,
    "sre_name": "Barbacena",
    "success": true,
    "licitacoes_found": 4,
    "duration_ms": 8000,
    "urls_scraped": 1
  }
}
```

**Se falhar:** Verificar logs do servidor no terminal

### Passo 1.2: Verificar Dados no Supabase â±ï¸ 3 min
```sql
-- No Supabase SQL Editor
SELECT 
  numero_edital,
  objeto,
  sre_code,
  regional,
  data_publicacao,
  created_at
FROM licitacoes
WHERE sre_code = 6
ORDER BY created_at DESC
LIMIT 10;
```

**OU usar Prisma Studio:**
```powershell
npm run prisma:studio
```
Abre UI visual em http://localhost:5555

### Passo 1.3: Testar MÃºltiplas SREs â±ï¸ 5 min
```powershell
# Teste com 3 SREs diferentes
Start-Process "http://localhost:3001/api/test-scraper?sre=6,13,25"
```

**Validar:**
- âœ… 3 SREs processadas
- âœ… Total de licitaÃ§Ãµes > 0
- âœ… Logs criados em `scraping_logs`
- âœ… Status das SREs atualizado

### Passo 1.4: Verificar Logs de Scraping â±ï¸ 2 min
```sql
-- Verificar Ãºltimos logs
SELECT 
  sre_code,
  sre_source,
  status,
  licitacoes_coletadas,
  execution_time_ms,
  started_at,
  error_message
FROM scraping_logs
ORDER BY started_at DESC
LIMIT 20;
```

---

## ğŸ¤– FASE 2: AGENTE DE ENRIQUECIMENTO IA (1-2 DIAS)

### Objetivo
Processar licitaÃ§Ãµes brutas e extrair dados estruturados usando **OpenRouter API** (Grok-4-fast).

### Passo 2.1: Criar Agente de Enriquecimento â±ï¸ 3 horas
**Arquivo:** `lib/agents/enrichment-agent.ts`

**Funcionalidades:**
- Ler licitaÃ§Ãµes com `processado_ia = false`
- Enviar `objeto` + `raw_data.text` para Grok-4-fast
- Extrair via prompt estruturado:
  - âœ… `escola` (nome da escola)
  - âœ… `municipio_escola`
  - âœ… `categoria_principal` (alimentaÃ§Ã£o, limpeza, materiais, etc)
  - âœ… `categorias_secundarias[]`
  - âœ… `itens_principais[]` (lista de produtos/serviÃ§os)
  - âœ… `palavras_chave[]`
  - âœ… `fornecedor_tipo` (MEI, ME, EPP, grande empresa)
  - âœ… `score_relevancia` (0-100)
  - âœ… `resumo_executivo` (2-3 linhas)
  - âœ… `complexidade` (baixa, mÃ©dia, alta)
- Salvar dados enriquecidos usando `prisma.licitacoes.update()`
- Marcar `processado_ia = true`

**Estrutura do prompt:**
```typescript
const prompt = `
Analise esta licitaÃ§Ã£o e extraia dados estruturados em JSON:

OBJETO: ${licitacao.objeto}
TEXTO COMPLETO: ${licitacao.raw_data?.text}

Retorne JSON no formato:
{
  "escola": "Nome da escola beneficiada",
  "municipio_escola": "MunicÃ­pio da escola",
  "categoria_principal": "alimentacao|limpeza|materiais|servicos|obras|outros",
  "categorias_secundarias": ["categoria1", "categoria2"],
  "itens_principais": ["item1", "item2", "item3"],
  "palavras_chave": ["palavra1", "palavra2"],
  "fornecedor_tipo": "MEI|ME|EPP|grande",
  "score_relevancia": 75,
  "resumo_executivo": "Resumo em 2-3 linhas",
  "complexidade": "baixa|media|alta"
}
`;
```

### Passo 2.2: Criar Endpoint de Processamento â±ï¸ 1 hora
**Arquivo:** `app/api/process-ia/route.ts`

```typescript
export async function POST(request: Request) {
  const { limit = 50 } = await request.json();
  
  // Buscar licitaÃ§Ãµes pendentes
  const licitacoes = await prisma.licitacoes.findMany({
    where: { processado_ia: false, objeto: { not: null } },
    orderBy: { created_at: 'desc' },
    take: limit
  });
  
  // Processar cada uma
  const results = [];
  for (const lic of licitacoes) {
    const enriched = await enrichLicitacao(lic);
    results.push(enriched);
  }
  
  return Response.json({ processed: results.length, results });
}
```

### Passo 2.3: Testar Enriquecimento â±ï¸ 30 min
```powershell
# Processar 10 licitaÃ§Ãµes
curl.exe -X POST http://localhost:3001/api/process-ia -H "Content-Type: application/json" -d "{\"limit\": 10}"
```

### Passo 2.4: Criar GitHub Action para Processamento DiÃ¡rio â±ï¸ 1 hora
**Arquivo:** `.github/workflows/enrich-daily.yml`

```yaml
name: Daily IA Enrichment
on:
  schedule:
    - cron: '30 7 * * *'  # 7:30 AM BRT (30 min apÃ³s scraping)
  workflow_dispatch:

jobs:
  enrich:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
        with:
          node-version: '20'
      - run: npm install
      - run: npx prisma generate
      - name: Process licitaÃ§Ãµes
        run: |
          curl -X POST ${{ secrets.VERCEL_URL }}/api/process-ia \
            -H "Content-Type: application/json" \
            -d '{"limit": 100}'
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
```

---

## ğŸ“± FASE 3: FRONTEND B2B PARA EMPRESAS (2-3 DIAS)

### Objetivo
Interface para fornecedores buscarem e filtrarem licitaÃ§Ãµes relevantes.

### Passo 3.1: PÃ¡gina de Dashboard â±ï¸ 4 horas
**Arquivo:** `app/empresas/page.tsx`

**Componentes:**
- ğŸ“Š Cards de estatÃ­sticas (total licitaÃ§Ãµes, valor mÃ©dio, prÃ³ximas a vencer)
- ğŸ“‹ Lista de Ãºltimas 20 licitaÃ§Ãµes enriquecidas
- ğŸ” Barra de busca rÃ¡pida
- ğŸ—ºï¸ Mapa com distribuiÃ§Ã£o por SRE
- ğŸ“ˆ GrÃ¡fico de categorias mais frequentes

### Passo 3.2: PÃ¡gina de LicitaÃ§Ãµes (Lista) â±ï¸ 5 horas
**Arquivo:** `app/empresas/licitacoes/page.tsx`

**Filtros:**
- â˜‘ï¸ Multi-select SREs (47 opÃ§Ãµes)
- â˜‘ï¸ Multi-select categorias
- â˜‘ï¸ Range de valor (R$ 0 - R$ 1.000.000+)
- â˜‘ï¸ Multi-select municÃ­pios
- â˜‘ï¸ Palavras-chave (busca full-text)
- â˜‘ï¸ Data de abertura (prÃ³ximos 7/15/30 dias)
- â˜‘ï¸ Complexidade (baixa, mÃ©dia, alta)

**Lista:**
- ğŸ“„ 50 itens por pÃ¡gina (paginaÃ§Ã£o)
- ğŸ¯ Score de relevÃ¢ncia visÃ­vel
- ğŸ’° Valor estimado destacado
- ğŸ“… Dias restantes atÃ© abertura
- ğŸ« Escola + municÃ­pio
- â­ BotÃ£o "Salvar" (favoritos em localStorage)
- ğŸ“¥ BotÃ£o "Exportar CSV"

### Passo 3.3: PÃ¡gina de Detalhes da LicitaÃ§Ã£o â±ï¸ 3 horas
**Arquivo:** `app/empresas/licitacao/[id]/page.tsx`

**SeÃ§Ãµes:**
- ğŸ“‹ Dados principais (nÃºmero, modalidade, objeto)
- ğŸ« Dados da escola (nome, municÃ­pio, endereÃ§o via IA)
- ğŸ“¦ Itens principais (lista formatada)
- ğŸ’° Valor estimado + complexidade
- ğŸ“… Cronograma (publicaÃ§Ã£o, abertura, prazo)
- ğŸ“ Documentos para download
- ğŸ¢ InformaÃ§Ãµes para contato
- ğŸ¤– Resumo executivo (gerado por IA)
- ğŸ”— Link para edital original

### Passo 3.4: Sistema de Alertas â±ï¸ 4 horas
**Arquivo:** `app/empresas/alertas/page.tsx`

**Funcionalidade:**
- ğŸ“§ Cadastro de email + preferÃªncias
- ğŸ¯ Definir filtros (mesmos da busca)
- ğŸ”” FrequÃªncia (diÃ¡rio, semanal)
- âœ… Ativar/desativar alertas
- ğŸ“Š HistÃ³rico de alertas enviados

**Backend:**
- Endpoint: `POST /api/alerts/create`
- Tabela: `user_alerts` (jÃ¡ existe no schema Prisma)
- Processamento: GitHub Action ou Vercel Cron

### Passo 3.5: Componentes ReutilizÃ¡veis â±ï¸ 2 horas
- `<FiltrosEmpresa>` - Sidebar com todos os filtros
- `<LicitacaoCard>` - Card individual na lista
- `<PaginationBar>` - Controles de paginaÃ§Ã£o
- `<ExportButton>` - Exportar resultados para CSV
- `<ScoreIndicator>` - Badge visual de relevÃ¢ncia
- `<ComplexidadeBadge>` - Ãcone de complexidade

---

## ğŸ”„ FASE 4: AUTOMAÃ‡ÃƒO COMPLETA (1 DIA)

### Passo 4.1: GitHub Action de Scraping DiÃ¡rio â±ï¸ 2 horas
**Arquivo:** `.github/workflows/scrape-daily.yml`

```yaml
name: Daily Scraping
on:
  schedule:
    - cron: '0 6 * * *'  # 6 AM BRT todos os dias
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        batch: [1, 2, 3, 4, 5]  # 5 batches paralelos
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
      - run: npm install
      - run: npx prisma generate
      - name: Scrape batch
        run: node scripts/scrape-batch.js ${{ matrix.batch }}
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
```

**Script:** `scripts/scrape-batch.js`
```javascript
const batch = process.argv[2];
const sresPerBatch = 10;
const start = (batch - 1) * sresPerBatch;
const end = start + sresPerBatch;

// Buscar SREs do batch
const sres = await prisma.sres.findMany({
  where: { ativo: true },
  orderBy: { codigo: 'asc' },
  skip: start,
  take: sresPerBatch
});

// Scrape
for (const sre of sres) {
  await scrapeSRE(sre);
}
```

### Passo 4.2: Monitoramento e Logs â±ï¸ 2 horas
- Dashboard em `/admin/logs` mostrando:
  - âœ… Taxa de sucesso por SRE (Ãºltimos 7 dias)
  - âœ… LicitaÃ§Ãµes coletadas (grÃ¡fico temporal)
  - âœ… Erros recentes
  - âœ… Tempo mÃ©dio de scraping
  - âœ… SREs com problemas (taxa < 50%)

### Passo 4.3: NotificaÃ§Ãµes de Erro â±ï¸ 1 hora
- Integrar com Discord/Slack/Email
- Alertar quando:
  - Taxa de sucesso < 50% por 3 dias
  - Nenhuma licitaÃ§Ã£o coletada em 24h
  - Erro crÃ­tico no scraping

---

## ğŸ“¦ FASE 5: DEPLOY E PRODUÃ‡ÃƒO (1 DIA)

### Passo 5.1: Configurar Vercel â±ï¸ 1 hora
```powershell
# Deploy via CLI
npm install -g vercel
vercel --prod
```

**Environment Variables:**
- `DATABASE_URL` (Connection Pooling)
- `OPENROUTER_API_KEY`
- `OPENROUTER_DEFAULT_MODEL=x-ai/grok-4-fast`

### Passo 5.2: Configurar Supabase ProduÃ§Ã£o â±ï¸ 1 hora
- Criar project produÃ§Ã£o (ou usar mesmo)
- Aplicar migrations: `npx prisma migrate deploy`
- Configurar Row Level Security (RLS)
- Backup automÃ¡tico habilitado

### Passo 5.3: DomÃ­nio Customizado â±ï¸ 30 min
- Configurar DNS (licitacoes-mg.com.br ou similar)
- Certificado SSL automÃ¡tico via Vercel

### Passo 5.4: Testes de Carga â±ï¸ 2 horas
- Simular 1000 requisiÃ§Ãµes simultÃ¢neas
- Verificar connection pooling
- Otimizar queries lentas
- Adicionar caching (Vercel Edge)

---

## ğŸ¯ PRIORIDADES IMEDIATAS (HOJE)

### âœ… PASSO 1: Validar Scraping (30 min)
```powershell
# 1. Abrir browser
Start-Process "http://localhost:3001/api/test-scraper?sre=6"

# 2. Ver dados
npm run prisma:studio

# 3. Testar mÃºltiplas SREs
Start-Process "http://localhost:3001/api/test-scraper?sre=6,13,25"
```

### âœ… PASSO 2: Criar Agente IA (AMANHÃƒ - 3h)
- Implementar `lib/agents/enrichment-agent.ts`
- Testar com 10 licitaÃ§Ãµes
- Validar dados enriquecidos

### âœ… PASSO 3: Frontend B2B (SEMANA QUE VEM - 2-3 dias)
- Dashboard empresas
- Lista com filtros
- PÃ¡gina de detalhes
- Sistema de alertas

---

## ğŸ“Š CRONOGRAMA COMPLETO

| Fase | DescriÃ§Ã£o | Tempo | Quando |
|------|-----------|-------|--------|
| âœ… 0 | Prisma ORM | 30 min | **HOJE** âœ… |
| â³ 1 | ValidaÃ§Ã£o | 30 min | **HOJE** |
| ğŸ”œ 2 | Agente IA | 1-2 dias | AmanhÃ£/Sexta |
| ğŸ“… 3 | Frontend B2B | 2-3 dias | Semana que vem |
| ğŸ“… 4 | AutomaÃ§Ã£o | 1 dia | Semana que vem |
| ğŸ“… 5 | Deploy | 1 dia | Semana que vem |

**Total**: 5-7 dias Ãºteis para MVP completo

---

## ğŸš¨ AÃ‡ÃƒO IMEDIATA

**Execute AGORA para validar que tudo estÃ¡ funcionando:**

```powershell
# 1. Abrir teste no browser
Start-Process "http://localhost:3001/api/test-scraper?sre=6"

# 2. Ver dados no Prisma Studio
npm run prisma:studio
```

**Se funcionar â†’** Seguir para Fase 2 (Agente IA)  
**Se falhar â†’** Debugar logs do servidor e me avisar

---

**Desenvolvido em**: 08/10/2025  
**Status atual**: Prisma implementado âœ… - Aguardando validaÃ§Ã£o
